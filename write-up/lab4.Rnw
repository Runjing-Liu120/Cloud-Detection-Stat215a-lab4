\documentclass{article}

\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{fancyhdr}
\pagestyle{fancy}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{algorithm,algorithmic}
\usepackage{subcaption}
\usepackage{color}
\usepackage{hyperref}
\usepackage{multirow}



\begin{document}

\title{Lab 4 - Cloud Detection}

\maketitle
<<setup, echo = FALSE>>=
# set defaults for future code chunks
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, 
                      dev='png', dpi = 300)
@


<<libraries, cache = FALSE>>=
library(tidyverse)
library(glmnet)
library(knitr)
library(ggplot2)
library(dplyr)
library(reshape2)

source('./R/data_processing_utils.R')
source('./R/lasso_utils.R')
source('./R/rf.R')
source('./R/SVM_utils.R')

blank_theme <- theme_minimal(base_size = 30) +
  theme(plot.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank())
remove_y <-   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())

@

<<load_data, cache = TRUE>>=
# first two columns are coordinates
# third columns are labels
# next eight are original features
# last eight are smoothed features 

path = '../image_data/'

image1.plots <- load_image(filename = 'image1.txt', path)
image2.plots <- load_image(filename = 'image2.txt', path)
image3.plots <- load_image(filename = 'image3.txt', path)

image1 <- prep_image(image1.plots)
image2 <- prep_image(image2.plots)
image3 <- prep_image(image3.plots)

@

\section{Introduction}
\subsection{Source of data (RN)}
- definitions/ data source
- expert labels plot
- models used to predict labels

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.3\textwidth}
<<image_plot1, cache = TRUE>>=
plotr(image1.plots, image1.plots$label, "none")
@
\subcaption{expert labeled image1}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
<<image_plot2, cache = TRUE>>=
plotr(image2.plots, image2.plots$label, "none") +
  remove_y
@
\subcaption{expert labeled image2}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
<<image_plot3, cache = TRUE>>=
plotr(image3.plots, image3.plots$label, "right") +
  remove_y
@
\subcaption{expert labeled image1}
\end{subfigure}
\caption{Expertly labeled figures}
\label{fig:expert_labels}
\end{figure}


\subsection{EDA (RN)}
- Differences between angle radiances
- cloud/nocloud angles
- cloud/nocloud 

\section{Models}
We describe here the four models used: logistic regression, LASSO, random forest, and support vector machines. We also describe the alternative smoothed features we considered. 

\subsection{Feature smoothing}\label{feature_smoothing}
In image data, neighboring pixels are generally not independent; in particular, there is usually be some ``continuity" in the structure of an image. Looking at figure~\ref{fig:expert_labels}, we see that a pixel labeled as a cloud is usually surrounded by other cloudy pixels; similarly, a pixel labeled as ice is usually surrounded by other icy pixels. 

Hence, similar to how the engineered features NDAI, CORR, and SD takes information from a surrouding $8\times 8$ grid of pixels, we also take in information from neighboring pixels. Instead of considering the original features, we consider features that are smoothed versions of the original: that is, for each pixel and each original feature, we add a feature that is the average of that original feature over its 64 nearest neighbors. 

We also hope that this smoothing may give better test accuracy by ameliorating the effect of outliers that may exist in the dataset. 

In our results, we will compare the predictive accuracy of algorithms that use the eight original features against those that use the smoothed features. 

\subsection{Logistic regression and LASSO}\label{lasso_descr}
As a first attempt to predict the expert labels, we fit a logistic regression using the original eight features, as well the smoothed features. 

However, to guard against overfitting, we then run a logistic regression but with an $L1$ penalty. The strength of this penalty was chosen by 2-fold cross validation, as described in section~\ref{CV_descr} below. In doing so, we hope that this regularization will give us better test accuracy by not overfitting to the training set, and it has the added benefit of selecting the important features needed for the prediction. For example, NDAI was noted in the paper  ({\color{red} cite}) and in our plots above ({\color{red} reference}) to be a potentially informative feature. We hope that this LASSO procedure will detect NDAI and other important features. 


\subsection{RF (YN) }\label{rf_descr}
We also trained several Random Forest models as classifiers to predict the image labels, using the original eight features and optionally the smoothed features. The number of decision trees in each model is chosen to be 500. We make the trees to return probabilities of the predicted labels instead of binary responses, and then apply thresholding (as we did in logisitic regression) to get the predicted labels.

\subsubsection*{Model selection and validation}
{\color{red} remove this section, we'll have a general model selection and validation section below. }

The model was first trained on image 1, and then the model was tested on image2 and image 3. The 2-fold cross validation was performed in that we also trained the model on image 3, and tested on image 1 and image 2. We use Random Forest as an out-of-the-box algorithm so there are no parameters to be tuned. 

\subsection{SVM (RN) }
Finally, we also used support vector machines for classification. However, we only used three features: NDAI, CORR, and DF, since these were features found to be important by the LASSO (see section~\ref{feature_selection}). We also fit the SVM with the smoothed version of these three features. The cost for constraint violation was chosen with 2-fold cross-validation as described in section~\ref{CV_descr}. 

\subsection{Model selection and validation}\label{CV_descr}
We used the first two images as our training set, and hold out the third image as a test set. Hence, all accuracy measures are given by predictions on this third test set. 

When parameters needed to be tuned, we used 2-fold cross-validation: we fit on the first image and validate on the second image; we then flip the procedure and fit to the second image and validate on the first. The predictive accuracy of these two folds are averaged, and we choose the parameter to maximize this averaged predictive accuracy. 

For SVM, we used this procedure to find the cost parameter for constraint violation; for LASSO, we used CV to find for the the strength of the L1 penalty. 

We chose to do 2-fold cross-validation since in image data, each pixel is likely not independent. A given pixel is likely to be similar to its neighbors; for example, from figure~\ref{fig:expert_labels} a cloud pixel is usually surrounded by by other cloudy pixels, and an ice pixel is usually surrounded by other ice pixels. Hence, if we randomly split an image into K-folds, these folds are not independent, and our accuracy measurement will be over-optimstic. Our choice to do 2-fold cross-validation rests on the assumption that while individual pixels are not independent, we assume that the two images are independent. We hope that this independence will give more realistic accuracy measurements. 



\section{Results}

\subsection{Predictive accuracy}
We present the predictive accuracy of using logistic regression, logistic-LASSO, random forest, and support vector machines. We examine the the difference in prediction accuracy on image2 and image3, and examine the benefits of having smoothed features. Our main results are summarized in table~\ref{tab:acc_results}. 


\subsubsection*{Logistic regression}

<<logistic_reg, cache = TRUE>>=
# fit logistic regression to first image
logistic_fit <- get_logistic_fit(image1, smooth_features = FALSE)
logistic_fit_smo <- get_logistic_fit(image1, smooth_features = TRUE)

# predict on second image
image2_pred <- get_logistic_prediction(logistic_fit, image2)
image2_pred_smo <- get_logistic_prediction(logistic_fit_smo, image2)

# predict on third image
image3_pred <- get_logistic_prediction(logistic_fit, image3)
image3_pred_smo <- get_logistic_prediction(logistic_fit_smo, image3)
@

As a baseline, we use a simple logistic regression on the original eight features to predict the expert labels. We fit the model to the first image, and examine predictive accuracy on the second and third images. With the original features, the predictive accuracy on the second image is \Sexpr{image2_pred$accuracy}, while the predictive accuracy on the third image is \Sexpr{image3_pred$accuracy}. 

We also fit a logistic regression on the smoothed features as described in section~\ref{smoothed_features}. Again fitting to image1, the predictive accuracies on images 2 and 3 are \Sexpr{image2_pred_smo$accuracy} and \Sexpr{image3_pred_smo$accuracy}, respectively. Therefore, it appears that smoothing the features did not have a significant effect when doing logistic regression.  

<<lasso, cache = TRUE, results = 'hide'>>=
lasso_fit <- get_lasso_fit_2fold_cv(image1, image2, smooth_features = FALSE)
lasso_fit_smo <- get_lasso_fit_2fold_cv(image1, image2, smooth_features = TRUE)

image3_lasso_pred <- get_lasso_prediction(lasso_fit$fit, image3)
image3_lasso_smo_pred <- get_lasso_prediction(lasso_fit_smo$fit, image3)
@

\subsubsection*{LASSO}
We now fit the logistic LASSO, which encourages sparsity in the regression coefficients.
 Our goal in doing so is two fold: firstly, we hope that the $L1$ penalty will help avoid overfitting to the training set; and secondly, we hope that the nonzero coefficients found by the LASSO will give insight to the important features needed for cloud detection. 
 
Unlike in the normal logistic regression, we here must do cross-validation to choose the regularization parameter. We do so using 2-fold CV as with image1 as the first fold and image2 as the second fold as described in section~\ref{lasso_descr}. The third image was held out as a test set. 

The predictive accuracy on the held out third image was \Sexpr{image3_lasso_pred$accuracy} when using the original features, and \Sexpr{image3_lasso_smo_pred$accuracy} when using the smoothed features. Like in the logistic regression, smoothing the features did not appear to make a big difference here. 

\subsubsection*{Random Forest}

<<rf, cache = TRUE, results = 'hide'>>=
# image1_neighbor <- add_neighbor_info(image1)
# image2_neighbor <- add_neighbor_info(image2)
# image3_neighbor <- add_neighbor_info(image3)

rf_fit_orig <- get_rf_fit(image1, smooth_features = FALSE)
rf_fit_orig_smo <- get_rf_fit(image1, smooth_features = TRUE)

image2_rf_pred <- get_rf_prediction(rf_fit_orig, image2.plots)
image2_rf_smo_pred <- get_rf_prediction(rf_fit_orig_smo, image2.plots)
image3_rf_pred <- get_rf_prediction(rf_fit_orig, image3.plots)
image3_rf_smo_pred <- get_rf_prediction(rf_fit_orig_smo, image3.plots)
@

<<rf_plot>>=
#plotr(image2.plots, fill = image2.plots$label)
#plotr(image2.plots, fill = image2_rf_pred$pred_class)
@

%Without feature smoothing, the predictive accuracy on image 2 was \Sexpr{image2_rf_pred$accuracy}, while on image 3 it was \Sexpr{image3_rf_pred$accuracy}. With feature smoothing, the predictive accuracy on image 2 was \Sexpr{image2_rf_smo_pred$accuracy}, while on image 3 it was \Sexpr{image3_rf_smo_pred$accuracy}.

Random forest models also reports feature importance by giving the Gini coefficient. In our original model which is trained on image 1 without smoothing, the top 3 most important features are \texttt{NDAI, AF} and \texttt{BF}. If we add the smoothing features, then 3 of the most important features would become \texttt{NDAI\_smoothed}, \texttt{AF\_smoothed} and \texttt{SD\_smoothed}.

Another group of experiments were conducted on a new dataset where each pixel would have access to the features of its neighboring pixels, i.e., the pixels to the up, down, left and right directions. If we train the model on image 1, then the top 3 of the most important features are \texttt{NDAI, NDAI\_u} and \texttt{NDAI\_l}, which suggests that the neighboring pixels provide more information than its local features.

\subsubsection*{Support Vector Machine}

<<svm_fit, cache = TRUE, results = 'hide'>>=
# warning: this block takes awhile
svm_fit <- get_svm_fit_2fold_cv(image1, image2, smooth_features = FALSE)
@

<<svm_fit_smo, cache = TRUE, results = 'hide'>>=
# warning: this block takes awhile
svm_fit_smo <- get_svm_fit_2fold_cv(image1, image2, smooth_features = TRUE)
@

<<svm_pred, cache = TRUE>>=
# why the fuck does 'predict' take so long too goddamnit
tmp <- 0
image3_svm_pred <- get_svm_prediction(svm_fit$fit, image3)
image3_svm_smo_pred <- get_svm_prediction(svm_fit_smo$fit, image3)
@

<<>>=
print(image3_svm_pred$accuracy)
print(image3_svm_smo_pred$accuracy)

@


\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
plotr(image3.plots, image3.plots$label, "right") +
  remove_y
@
\subcaption{expert labeled image3}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
plotr(image3.plots, image3.plots$label, "right") +
  remove_y
@
\subcaption{lasso prediction on image 3}
\end{subfigure}\\
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
knitr::include_graphics('./panda.jpg')
@
\subcaption{SVM prediction on image 3}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
knitr::include_graphics('./panda.jpg')
@
\subcaption{RF prediction on image 3}
\end{subfigure}
\caption{Expertly labeled figures}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{cc|cccc}
 & & \multicolumn{4}{c}{Predictive accuracy} \\
 & & logistic regression & LASSO & RF & SVM \\
    \hline
    \multirow{2}{*}{Image2}& smoothed features & 
    \Sexpr{image2_pred_smo$accuracy} & NA & 100 & 100\\
    & original features & 
    \Sexpr{image2_pred$accuracy} & NA & 100 & 100\\
    \hline
    \hline
    \multirow{2}{*}{Image3}& smoothed features & 
    \Sexpr{image3_pred_smo$accuracy} & \Sexpr{image3_lasso_smo_pred$accuracy} & 100 & 100\\
    &original features & 
    \Sexpr{image3_pred$accuracy} & \Sexpr{image3_lasso_pred$accuracy} & 100 & 100\\
    \hline
\end{tabular}
\caption{accuracy on test set}
\label{tab:acc_results}
\end{table}

\subsection{Feature selection}\label{feature_selection}
<<lasso_feat_selection, cache = TRUE, results = 'hide'>>=
image12 <- rbind(image1, image2)

labeled_indx <- which(image12$label != 0)
X <- image12[labeled_indx, -c(1,2,3)] # use both smoothed and unsmoothed features
y <- droplevels(image12$label[labeled_indx])

fit_lasso12_path <- glmnet(as.matrix(X), y, family = "binomial")

lasso_fit_all_feat <- get_lasso_fit_2fold_cv(image1, image2, smooth_features = NULL)
@
The $L1$ penalty in the LASSO not only helps prevent overfitting, but the sparsity pattern also gives insight into which features are truly important for predicting cloud coverage. With this goal in mind, we fit the LASSO on the first two images with 16 features: the original 8 plus their smoothed versions, and see which regression coefficients are nonzero. The coefficients as a function of the penalty parameter is shown in figure~\ref{fig:lasso_penalty}. We choose the penalty parameter using 2-fold cross-validation with the two images as folds (see section~\ref{lasso_descr}) to choose an optimal lambda. At this optimal lambda, we see that only four features are active: smoothed NDAI, smoothed CORR, smoothed DF, and non-smoothed CORR, with the non-smoothed CORR coefficient being barely non-zero. Hence, this suggests that NDAI, CORR, and DF are the three most important features, and moreover, looking at the smoothed versions may be sufficient. 

\begin{figure}[h]
\centering
\begin{subfigure}{0.4\textwidth}
<<lasso_path, cache = TRUE, fig.height=5, fig.width=6.5>>=
coeff <- as.matrix(fit_lasso12_path$beta)
feature_names <- rownames(coeff)

coeff_vec <- as.vector(coeff)
lambdas <- fit_lasso12_path$lambda

coeff_df <- data.frame(coefficient = coeff_vec, 
                       name = rep(feature_names, 100), 
                       log_lambda = log(as.vector(t(replicate(16, lambdas)))))

ggplot(coeff_df) + geom_line(aes(x = log_lambda, y = coefficient, color = name)) + 
  geom_vline(xintercept=log(lasso_fit_all_feat$cv_results$lambda_best), 
             linetype="dotted") + 
  annotate('text', x = -7.8, y = 4, label = "NDAI_smoothed", size = 4) + 
  annotate('text', x = -5.2, y = 1.75, label = "CORR_smoothed", size = 4) + 
  annotate('text', x = -4.8, y = 0.3, label = "DF_smoothed",  size = 4) + 
  theme(legend.position="none")
@
\end{subfigure}
\caption{The LASSO regression path; that is, we plot the values of the regression coefficients of the 16 features (eight original, eight smoothed) as a function of the penalty parameter $\lambda$. The vertical dotted line is the optimal $\lambda$ chosen by cross-validation. At this point, we see that there are three clearly non-zero coefficients: smoothed NDAI, smoothed CORR, and smoothed DF. }
\label{fig:lasso_penalty}
\end{figure}


Doing RF we found blah blah features to be important. 

Hence SVM only used these blah blah features. 

\section{Discussion}
- final model chosen: Show some diagnostic plots or information related to convergence or
parameter estimation.
For your best classification model(s), do you notice any patterns in the misclassification errors? Again,
use quantitative and visual methods of analysis. Do you notice problems in particular regions, or in
specific ranges of feature values?
6. How well do you think your model will work on future data without expert labels?


\end{document}
