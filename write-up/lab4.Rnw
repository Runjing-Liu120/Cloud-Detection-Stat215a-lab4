\documentclass{article}

\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{fancyhdr}
\pagestyle{fancy}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{algorithm,algorithmic}
\usepackage{subcaption}
\usepackage{color}
\usepackage{hyperref}


\begin{document}

\title{Lab 4 - Cloud Detection}

\maketitle
<<setup, echo = FALSE>>=
# set defaults for future code chunks
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, 
                      dev='png', dpi = 300)
@


<<libraries>>=
library(tidyverse)
library(glmnet)

source('./R/data_processing_utils.R')
source('./R/lasso_utils.R')

@

<<load_data>>=
# first two columns are coordinates
# third columns are labels
# next eight are original features
# last eight are smoothed features 
image1 <- prep_image(filename = 'image1.txt', path = '../image_data/')
image2 <- prep_image(filename = 'image2.txt', path = '../image_data/')
image3 <- prep_image(filename = 'image3.txt', path = '../image_data/')
@

\section{Introduction}
\subsection{Source of data (RN)}
- definitions/ data source
- expert labels plot
- models used to predict labels

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
knitr::include_graphics('./panda.jpg')
@
\subcaption{expert labeled image1}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
knitr::include_graphics('./panda.jpg')
@
\subcaption{expert labeled image2}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
knitr::include_graphics('./panda.jpg')
@
\subcaption{expert labeled image1}
\end{subfigure}
\caption{Expertly labeled figures}
\label{fig:expert_labels}
\end{figure}

\subsection{EDA (RN)}
- Differences between angle radiances
- cloud/nocloud angles
- cloud/nocloud 

\section{Models}

\subsection{Feature smoothing}
In image data, neighboring pixels are generally not independent; in particular, there is reason to believe there to be some ``continuity" in the structure of an image. Looking at figure~\ref{fig:expert_labels}, we see that a pixel labeled as a cloud, is usually surrounded by other cloudy pixels; similarly, a pixel labeled as ice is usually surrounded by other icy pixels. 

Hence, similar to how the engineered features NDAI, CORR, and SD in ({\color{red} cite}) at a pixel takes information from its surrouding $8\times 8$ grid of pixels, we also smooth the features. In addition to the original eight features, we add eight more feautres that are smoothed versions of the original: that is, for each pixel and each original feature, we add a feature that is the average of that original feature over its 64 nearest neighbors. 

In our results, we will compare the predictive accuracy of algoirithms that only use the eight original features against those that use the smoothed features. 

\subsection{Logistic regression and LASSO}
As a first attempt to predict the expert labels, we fit a logistic regression using the eight features, as well as the eight features plus their smoothed their smoothed values. 
However, to guard against overfitting, we then fit a logistic regression to those features described above but with an $L1$ penalty. In doing so, we hope that this regularization will give us better test accuracy by not overfitting to the training set, and it has the added benefit of selecting the important features needed for the prediction. For example, NDAI was noted in the paper  ({\color{red} cite}) and in our plots above ({\color{red} reference}) to be a potentially informative feature. We hope that this LASSO procedure will detect NDAI and other important features. 

\subsubsection{Model selection and validation}
The regularization parameter was chosen using 2-fold cross-validation: we fit on the first image and validate on the second image; we then flip the procedure and fit to the second image and validate on the first. The predictive accuracy of these two folds are averaged, and we choose the optimal regularization parameter to maximize this averaged predictive accuracy. 

We chose to do 2-fold cross-validation since in image data, each pixel is likely not independent. A given pixel is likely to be similar to its neighbors; for example, from figure~\ref{fig:expert_labels} a cloud pixel is usually surrounded by by other cloudy pixels, and an ice pixel is usually surrounded by other ice pixels. Hence, if we randomly split an image into K-folds, these folds are not independent, and our accuracy measurement will be over-optimstic. Our choice to do 2-fold cross-validation rests on the assumption that while individual pixels are not independent, we assume that the two images are independent. We hope that this independence will give more realistic accuracy measurements. 

\subsection{RF (YN) }

\subsubsection{Model selection and validation}
describe how CV was done (if at all), and what were your test and training sets. 

\subsection{SVM (RN) }
\subsubsection{Model selection and validation}
describe how CV was done (if at all), and what were your test and training sets. 



\section{Results}
\subsection{Feature selection}
lasso coefficients selected ...
random forest feature importance ...

\subsection{Predictive accuracy}
predictive accuracy on second image vs. thrid image. 
predictive accuracy on smoothed vs. non-smoothed features. 

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
knitr::include_graphics('./panda.jpg')
@
\subcaption{expert labeled image3}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
knitr::include_graphics('./panda.jpg')
@
\subcaption{lasso prediction on image 3}
\end{subfigure}\\
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
knitr::include_graphics('./panda.jpg')
@
\subcaption{SVM prediction on image 3}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
knitr::include_graphics('./panda.jpg')
@
\subcaption{RF prediction on image 3}
\end{subfigure}
\caption{Expertly labeled figures}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{lllll}
& logistic & LASSO & RF & SVM \\
smoothed features & 0.7258 & 0.7182 !! & 100\% !! & 100\% !!\\
original features & 0.7271 & 0.7180 & 99\% !! & 99\% !!
\end{tabular}
\caption{accuracy on test set}
\end{table}

\subsection{Features}
- quantitative and visual justification

\section{Discussion}
- final model chosen: Show some diagnostic plots or information related to convergence or
parameter estimation.
For your best classification model(s), do you notice any patterns in the misclassification errors? Again,
use quantitative and visual methods of analysis. Do you notice problems in particular regions, or in
specific ranges of feature values?
6. How well do you think your model will work on future data without expert labels?


\end{document}
