\documentclass{article}

\usepackage{geometry}
\usepackage[backend=bibtex]{biblatex}
\bibliography{references}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{fancyhdr}
\pagestyle{fancy}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{algorithm,algorithmic}
\usepackage{subcaption}
\usepackage{color}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{float}
\usepackage{wrapfig}


\begin{document}

\title{Lab 4 - Cloud Detection}

\maketitle
<<setup, echo = FALSE>>=
# set defaults for future code chunks
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, 
                      dev='png', dpi = 300)
@


<<libraries, cache = FALSE>>=
library(tidyverse)
library(glmnet)
library(knitr)
library(ggplot2)
library(dplyr)
library(reshape2)
library(GGally)

source('./R/data_processing_utils.R')
source('./R/lasso_utils.R')
source('./R/rf.R')
source('./R/SVM_utils.R')

blank_theme <- theme_minimal(base_size = 30) +
  theme(plot.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank())

blank_theme_small <- theme_minimal(base_size = 20) +
  theme(plot.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank())
        
remove_y <-   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
@

<<load_data, cache = TRUE>>=
# first two columns are coordinates
# third columns are labels
# next eight are original features
# last eight are smoothed features 

path = '../image_data/'

image1.plots <- load_image(filename = 'image1.txt', path)
image2.plots <- load_image(filename = 'image2.txt', path)
image3.plots <- load_image(filename = 'image3.txt', path)

image1 <- prep_image(image1.plots)
image2 <- prep_image(image2.plots)
image3 <- prep_image(image3.plots)

ones <- image1.plots %>%
  filter(label == 1)

@

\section{Introduction}
In this report, we investigate the application of various statistical and machine learning models for a task in image recognition. Specifically, we apply logistic regression, LASSO, random forest, and support vector machines in distinguishing between pixels representing either cloud or ice cover from satellite imagery. We build on the work presented in {\color{red} cite, name the paper}, which utilizes thresholding on three features of the imagery through an enhanced linear correlation matching (ELCM) algorithm. The images used to train and test our models are from the National Aeronautics and Space Administration's (NASA) Multiangle Imaging SpectroRadiometer (MISR). In light of the climate change-driven melting of the ice caps in the polar regions, it becomes increasingly important and difficult to monitor ice cover remotely, provided feedback effects increasing the frequency of cloud cover.
We begin our report with an exploration and analysis of three MISR images and their features.

\subsection{Image Properties}

The MISR images 
Each image dataframe includes the following recordings for each pixel: its x,y placement within the image, an expert labeling of either 1, 0, -1 (cloud, unclear, and no cloud, respectively), red-band radiance measurments from five different angles, the correlation of MISR images of the same scene from different MISR viewing directions (CORR), the standard deviation of the MISR nadir camera pixel values accross a scene (SD), and the normalized difference angular index (NDAI). We consider the five different radiance measurements, CORR, SD, and NDAI as features for our model. 


multi-angle and recorded in the red-band



\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.3\textwidth}
<<image_plot1, cache = TRUE>>=
plotr(image1.plots, image1.plots$label, "none")
@
\subcaption{expert labeled image1}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
<<image_plot2, cache = TRUE>>=
plotr(image2.plots, image2.plots$label, "none") +
  remove_y
@
\subcaption{expert labeled image2}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
<<image_plot3, cache = TRUE>>=
plotr(image3.plots, image3.plots$label, "right") +
  remove_y
@
\subcaption{expert labeled image1}
\end{subfigure}
\caption{Expertly labeled figures}
\label{fig:expert_labels}
\end{figure}



\subsubsection{Angles}





\begin{wrapfigure}{R}{0.6\textwidth}
  \begin{center}
<<image_plot100, cache = TRUE, fig.align='center', out.width='0.6\\textwidth'>>=
suppressMessages(require(ggplot2))
ggpairs(ones[, 7:11], lower=list(continuous = wrap("smooth", alpha = 0.2, fill = NA)),
        diag=list(continuous="barDiag"),
        upper = list(corSize=12)) + blank_theme_small
@
  \end{center}
\end{wrapfigure}



- Differences between angle radiance

In this section we explore the relationship of the different radiance measures besed on the different angles: DF, CF, BF, AF, and AN. The most forward-viewing oblique camera is DF and the most aft-viewing oblique camera is AN, with the progression between the two being representing by the degression of the alphabetical labeling. The radiance value distribution and correlations between the different angles is summarized in the figure to the right. Here we see that the similarity between the different angle measures is inconsistent. The inconsitency can be attributed to the change in the positioning of the clouds as each angle measure is taken. This issue is addressed by {\color{red} cite, name the paper}. While the ground pixels recordings remain rather consitent as the satellite travels over the region, the clouds both naturally shift and are obscured by the refraction angles.



\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.27\textwidth}
<<image_plot5, cache = TRUE>>=
ggplot(image1.plots) + 
  geom_density(aes(x = DF, group = label, fill = label), 
               alpha = 0.5) +
  blank_theme +
  theme(legend.position="none")
@
\subcaption{expert labeled image1}
\end{subfigure}
\begin{subfigure}[t]{0.27\textwidth}
<<image_plot6, cache = TRUE>>=
ggplot(image1.plots) + 
  geom_density(aes(x = CF, group = label, fill = label), 
               alpha = 0.5) +
  blank_theme +
  theme(legend.position="none",
        axis.title.y=element_blank())
@
\subcaption{expert labeled image2}
\end{subfigure}
\begin{subfigure}[t]{0.27\textwidth}
<<image_plot7, cache = TRUE>>=
ggplot(image1.plots) + 
  geom_density(aes(x = BF, group = label, fill = label), 
               alpha = 0.5)  +
  blank_theme +
  guides(color=guide_legend("Label",
    override.aes = list(size=10))) +
  theme(axis.title.y=element_blank())
@
\subcaption{expert labeled image1}
\end{subfigure}
\begin{subfigure}[t]{0.27\textwidth}
<<image_plot8, cache = TRUE>>=
ggplot(image1.plots) + 
  geom_density(aes(x = AF, group = label, fill = label), 
               alpha = 0.5)  +
  blank_theme +
  guides(color=guide_legend("Label",
    override.aes = list(size=10))) +
  theme(legend.position="none")
@
\subcaption{expert labeled image1}
\end{subfigure}
\begin{subfigure}[t]{0.27\textwidth}
<<image_plot9, cache = TRUE>>=
ggplot(image1.plots) + 
  geom_density(aes(x = AN, group = label, fill = label), 
               alpha = 0.5)  +
  blank_theme +
  guides(color=guide_legend("Label",
    override.aes = list(size=10))) +
  theme(axis.title.y=element_blank())
@
\subcaption{expert labeled image1}
\end{subfigure}
\caption{Expertly labeled figures}
\label{fig:cloud/nocloud}
\end{figure}

\subsubsection{Additional Features}

- cloud/nocloud 

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.27\textwidth}
<<image_plot10, cache = TRUE>>=
ggplot(image1.plots) + 
  geom_density(aes(x = NDAI, group = label, fill = label), 
               alpha = 0.5) +
  blank_theme +
  theme(legend.position="none")
@
\subcaption{expert labeled image1}
\end{subfigure}
\begin{subfigure}[t]{0.27\textwidth}
<<image_plot11, cache = TRUE>>=
ggplot(image1.plots) + 
  geom_density(aes(x = CORR, group = label, fill = label), 
               alpha = 0.5) +
  blank_theme +
  theme(legend.position="none",
        axis.title.y=element_blank())
@
\subcaption{expert labeled image2}
\end{subfigure}
\begin{subfigure}[t]{0.27\textwidth}
<<image_plot12, cache = TRUE>>=
ggplot(image1.plots) + 
  geom_density(aes(x = SD, group = label, fill = label), 
               alpha = 0.5)  +
  blank_theme +
  guides(color=guide_legend("Label",
    override.aes = list(size=10))) +
  theme(axis.title.y=element_blank())
@
\subcaption{expert labeled image1}
\end{subfigure}
\caption{Expertly labeled figures}
\label{fig:cloud/nocloud 
}
\end{figure}


\subsection{Feature smoothing}\label{feature_smoothing}
In our analysis, we will consider not only the eight features, but also smoothed versions of them. We do this because in image data, neighboring pixels are generally not independent; in particular, there is usually be some ``continuity" in the structure of an image. Looking at figure~\ref{fig:expert_labels}, we see that a pixel labeled as a cloud is usually surrounded by other cloudy pixels; similarly, a pixel labeled as ice is usually surrounded by other icy pixels. 

Hence, inspired by how the engineered features NDAI, CORR, and SD take radiances from a surrouding $8\times 8$ grid of pixels, we also take in information from neighboring pixels. Instead of considering the original features, we consider features that are smoothed versions of the original: that is, for each pixel and each original feature, we add a feature that is the average of that original feature over its 64 nearest neighbors. 

We also hope that this smoothing may give better test accuracy by ameliorating the effect of outliers that may exist in the dataset. 

In our results, we will compare the predictive accuracy of algorithms that use the eight original features against those that use the smoothed features. 


\section{Models}
We describe here the four algorithms used: logistic regression, logistic LASSO, random forest, and support vector machines. We only consider binary classification, with the two classes being cloud or ice; we ignore the unlabeled pixels where the expert was uncertain. 

\subsection{Logistic regression and LASSO}\label{lasso_descr}
As a first attempt to predict the expert labels, we fit a logistic regression using the first two images as our training set. 

However, to guard against overfitting, we then tried the logistic LASSO; that is, we run a logistic regression with an $L1$ penalty. The strength of this penalty was chosen by 2-fold cross validation using the first two images, as described in section~\ref{CV_descr} below. By doing LASSO, we hope that the regularization will give us better test accuracy by not overfitting to the training set, and the sparsity has the added benefit of suggesting which features are necessary for the prediction. For example, NDAI was noted in the paper \cite{shi2008daytime} and in our plots above ({\color{red} reference}) to be a potentially informative feature. We hope that this LASSO procedure will detect NDAI and other important features. 


\subsection{RF}\label{rf_descr}
Since the paper \cite{shi2008daytime} made its predictions by thresholding the features, it seemed natural to see if a random forest could make better predictions, since a random forest essentially uses multiple layers of thresholding to partition the feature space. 

Our random forest was trained on the first two images. At each node, 2 features were tested to determine the split and trees were grown to purity. The forest consisted of 500 trees. 

% We make the trees to return probabilities of the predicted labels instead of binary responses, and then apply thresholding (as we did in logisitic regression) to get the predicted labels.
% 
% \subsubsection*{Model selection and validation}
% {\color{red} remove this section, we'll have a general model selection and validation section below. }
% 
% The model was first trained on image 1, and then the model was tested on image2 and image 3. The 2-fold cross validation was performed in that we also trained the model on image 3, and tested on image 1 and image 2. We use Random Forest as an out-of-the-box algorithm so there are no parameters to be tuned. 

\subsection{SVM}
Finally, we also used support vector machines for classification. However, we only gave the SVM three features: NDAI, CORR, and DF, since these were features found to be important by the LASSO (see section~\ref{feature_selection}). The cost for constraint violation was chosen with 2-fold cross-validation as described in section~\ref{CV_descr}. 

\subsection{Model selection and validation}\label{CV_descr}
We used the first two images as our training set, and hold out the third image as a test set. To measure prediction accuracies, we only use the pixels with expert labels, ignoring the unlabeled pixels (since we are doing binary classification).

When parameters needed to be tuned, we used 2-fold cross-validation the first two images (our training set) with an entire image as one fold. In other words, we fit on the first image and validate on the second image; we then flip the procedure and fit to the second image and validate on the first. The predictive accuracy of these two folds are averaged, and we choose the parameter to maximize this averaged predictive accuracy. 

For SVM, we used this procedure to find the cost parameter for constraint violation; for LASSO, we used CV to find for the the strength of the L1 penalty. 

We chose to do 2-fold cross-validation since in image data, each pixel is likely not independent. A given pixel is likely to be similar to its neighbors; for example, from figure~\ref{fig:expert_labels} a cloud pixel is usually surrounded by by other cloudy pixels, and an ice pixel is usually surrounded by other ice pixels. Hence, if we randomly split an image into K-folds, these folds are not independent, and our accuracy measurement will be over-optimstic. Our choice to do 2-fold cross-validation rests on the assumption that while individual pixels are not independent, it may be more realistic to believe that the two images are independent. We hope that this independence will give more conservative training error measurements, and help avoid overfitting. In the end, the success of our procedures will be evaluated by predicting on the held out third image.  

\section{Results}

\subsection{Predictive accuracy}
We present the predictive accuracy of using logistic regression, logistic-LASSO, random forest, and support vector machines, and compare the benefits, if any, of using the smoothed features instead of the original features. Since we are only doing binary classification, accuracy is measured by the success of predicting the expert labeled pixels; we ignore the unlabeled pixels. Our predictive accuracies are summarized in table~\ref{tab:acc_results}, and figures~\ref{fig:map_orig_features} and \ref{fig:map_smo_features} visually compares the expert labels with the predicted labels. 

\subsubsection*{Logistic regression}

<<logistic_reg, cache = TRUE>>=

# fit logistic regression to first two images
image12 <- rbind(image1, image2)
logistic_fit <- get_logistic_fit(image12, smooth_features = FALSE)
logistic_fit_smo <- get_logistic_fit(image12, smooth_features = TRUE)

# predict on third image
image3_pred <- get_logistic_prediction(logistic_fit, image3.plots)
image3_pred_smo <- get_logistic_prediction(logistic_fit_smo, image3.plots)
@

As a baseline, we fit a simple logistic regression. We train the model using the first and second images, and examine the predictive accuracy on the third image. Using the original features, our predictive accuracy is \Sexpr{image3_pred$accuracy}. Using the smoothed features, we get an accuracy of \Sexpr{image3_pred_smo$accuracy}. Therefore, it appears that smoothing the features did not have a significant effect when doing logistic regression.  

\subsubsection*{LASSO}

<<lasso, cache = TRUE, results = 'hide'>>=
# fit lasso using 2-fold CV on first two images
lasso_fit <- get_lasso_fit_2fold_cv(image1, image2, smooth_features = FALSE)
lasso_fit_smo <- get_lasso_fit_2fold_cv(image1, image2, smooth_features = TRUE)

# predict on third image
image3_lasso_pred <- get_lasso_prediction(lasso_fit$fit, image3.plots)
image3_lasso_smo_pred <- get_lasso_prediction(lasso_fit_smo$fit, image3.plots)
@

We now examine the success of the LASSO, trained on the first two images with cross validation as described in section~\ref{CV_descr}. The predictive accuracy on the held out third image was \Sexpr{image3_lasso_pred$accuracy} when using the original features, and \Sexpr{image3_lasso_smo_pred$accuracy} when using the smoothed features. Like in the logistic regression, smoothing the features did not appear to make a big difference here. 

\subsubsection*{Random Forest}

<<rf, cache = TRUE, results = 'hide'>>=
# image1_neighbor <- add_neighbor_info(image1)
# image2_neighbor <- add_neighbor_info(image2)
# image3_neighbor <- add_neighbor_info(image3)

# train on images 1 and 2
rf_fit_orig <- get_rf_fit(image12, smooth_features = FALSE)
rf_fit_orig_smo <- get_rf_fit(image12, smooth_features = TRUE)
@

<<rf_pred, cache = TRUE, results = 'hide'>>=
# predict on image 3

image3_rf_pred <- get_rf_prediction(rf_fit_orig, image3.plots)
image3_rf_smo_pred <- get_rf_prediction(rf_fit_orig_smo, image3.plots)
@

After training the random forest on the first two images with the original features, the predictive accuracy on image 3 was \Sexpr{image3_rf_pred$accuracy}. With feature smoothing, the predictive accuracy on image 3 was \Sexpr{image3_rf_smo_pred$accuracy}. Smoothing may have helped slightly here. 

%Random forest models also reports feature importance by giving the Gini coefficient. In our original model which is trained on image 1 without smoothing, the top 3 most important features are \texttt{NDAI, AF} and \texttt{BF}. If we add the smoothing features, then 3 of the most important features would become \texttt{NDAI\_smoothed}, \texttt{AF\_smoothed} and \texttt{SD\_smoothed}.

%Another group of experiments were conducted on a new dataset where each pixel would have access to the features of its neighboring pixels, i.e., the pixels to the up, down, left and right directions. If we train the model on image 1, then the top 3 of the most important features are \texttt{NDAI, NDAI\_u} and \texttt{NDAI\_l}, which suggests that the neighboring pixels provide more information than its local features.

\subsubsection*{Support Vector Machine}

<<svm_fit, cache = TRUE, results = 'hide'>>=
# warning: this block takes awhile

# fit SVM using 2-fold CV on first two images
svm_fit <- get_svm_fit_2fold_cv(image1, image2, smooth_features = FALSE)
@

<<svm_fit_smo, cache = TRUE, results = 'hide'>>=
# warning: this block takes awhile

# fit SVM using 2-fold CV on first two images
# use smooth features
svm_fit_smo <- get_svm_fit_2fold_cv(image1, image2, smooth_features = TRUE)
@

<<svm_pred, cache = TRUE>>=
# why the fuck does 'predict' take so long too 

# predict on third image
image3_svm_pred <- get_svm_prediction(svm_fit$fit, image3.plots)
image3_svm_smo_pred <- get_svm_prediction(svm_fit_smo$fit, image3.plots)
@

Finally, we examine prediction using an SVM. As described in section \ref{CV_descr}, the first two images were used for training, and 2-fold cross-validation was used to set the cost of contraint violation. 

Here, we only fit using 3 features: NDAI, CORR, and DF, variables found to be important by the LASSO (see section~\ref{feature_selection}). Using these tree features, the predictive accuacy on the held out third image was \Sexpr{image3_svm_pred$accuracy}; after smoothing these three features, the predictive accuracy was \Sexpr{image3_svm_smo_pred$accuracy}.  

\begin{table}[tb]
 \centering
 \begin{tabular}{l|llll}
 & logistic & LASSO & RF & SVM \\\hline
 smoothed features & \Sexpr{image3_pred_smo$accuracy} &
 \Sexpr{image3_lasso_smo_pred$accuracy}& \Sexpr{image3_rf_smo_pred$accuracy} & \Sexpr{image3_svm_smo_pred$accuracy}\\
 original features & \Sexpr{image3_pred$accuracy} &  \Sexpr{image3_lasso_pred$accuracy} & \Sexpr{image3_rf_pred$accuracy}& \Sexpr{image3_svm_pred$accuracy}
 \end{tabular}
\caption{accuracy on test set}
\label{tab:acc_results}
\end{table}

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.245\textwidth}
<<cache = TRUE>>=
plotr(image3.plots, image3.plots$label, "none") +
  remove_y + ggtitle('Truth')
@
\subcaption{}
\end{subfigure}
\begin{subfigure}[t]{0.245\textwidth}
<<cache = TRUE>>=
plotr(image3.plots, image3_lasso_pred$pred_class, "none") +
  remove_y + ggtitle('LASSO')
@
\subcaption{}
\end{subfigure}
\begin{subfigure}[t]{0.245\textwidth}
<<cache = TRUE>>=
plotr(image3.plots, image3_rf_pred$pred_class, "none") +
  remove_y + ggtitle('RF')
@
\subcaption{}
\end{subfigure}
\begin{subfigure}[t]{0.245\textwidth}
<<cache = TRUE>>=
plotr(image3.plots, image3_svm_pred$pred_class, "none") +
  remove_y + ggtitle('SVM')
@
\end{subfigure}
\caption{Labels on image 3, using the original eight features. Fits were done with logistic-LASSO, random forest, and support vector machines. }\label{fig:map_orig_features}
\end{figure}

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.245\textwidth}
<<cache = TRUE>>=
plotr(image3.plots, image3.plots$label, "none") +
  remove_y + ggtitle('Truth')
@
\subcaption{}
\end{subfigure}
\begin{subfigure}[t]{0.245\textwidth}
<<cache = TRUE>>=
plotr(image3.plots, image3_lasso_smo_pred$pred_class, "none") +
  remove_y + ggtitle('LASSO')
@
\subcaption{}
\end{subfigure}
\begin{subfigure}[t]{0.245\textwidth}
<<cache = FALSE>>=
plotr(image3.plots, image3_rf_smo_pred$pred_class, "none") +
  remove_y + ggtitle('RF')
@
\subcaption{}
\end{subfigure}
\begin{subfigure}[t]{0.245\textwidth}
<<cache = TRUE>>=
plotr(image3.plots, image3_svm_smo_pred$pred_class, "none") +
  remove_y + ggtitle('SVM')
@
\subcaption{}
\end{subfigure}
\caption{Labels on image 3, using smoothed features. Fits were done with logistic-LASSO, random forest, and support vector machines. }\label{fig:map_smo_features}
\end{figure}

\subsection{Feature selection}\label{feature_selection}
<<axis_sizes, cache = TRUE>>=
title_size <- 25
axis_title_size <- 20
axis_element_size <- 20

set_axis_size <- theme(axis.text=element_text(size=axis_element_size), 
                       axis.title = element_text(size=axis_title_size), 
                       plot.title = element_text(size=title_size))

@

<<lasso_feat_selection, cache = TRUE, results = 'hide'>>=
# fit lasso on all 16 features
# we shall examine the sparisity in the next chunk

image12 <- rbind(image1, image2)

labeled_indx <- which(image12$label != 0)
X <- image12[labeled_indx, -c(1,2,3)] # use both smoothed and unsmoothed features
y <- droplevels(image12$label[labeled_indx])

fit_lasso12_path <- glmnet(as.matrix(X), y, family = "binomial")

lasso_fit_all_feat <- get_lasso_fit_2fold_cv(image1, image2, smooth_features = NULL)
@
The $L1$ penalty in the LASSO not only helps prevent overfitting, but the sparsity pattern also gives insight into which features are truly important for predicting cloud coverage. With this goal in mind, we fit the LASSO on the first two images with 16 features, the original 8 plus their smoothed versions, and see which regression coefficients are nonzero. The coefficients as a function of the penalty parameter is shown in figure~\ref{fig:lasso_penalty}. We choose the penalty parameter using 2-fold cross-validation with the two images as folds (see section~\ref{lasso_descr}). At this optimal lambda, we see that only four features are active: smoothed NDAI, smoothed CORR, smoothed DF, and non-smoothed CORR, with the non-smoothed CORR coefficient being barely non-zero. Hence, this suggests that NDAI, CORR, and DF are the three most important features, and moreover, looking at the smoothed versions may be sufficient. 

\begin{figure}[h]
\centering
\begin{subfigure}{0.6\textwidth}
<<lasso_path, cache = TRUE, fig.height=5, fig.width=6.5>>=
# get lasso fit path

coeff <- as.matrix(fit_lasso12_path$beta)
feature_names <- rownames(coeff)

coeff_vec <- as.vector(coeff)
lambdas <- fit_lasso12_path$lambda

coeff_df <- data.frame(coefficient = coeff_vec, 
                       name = rep(feature_names, 100), 
                       log_lambda = log(as.vector(t(replicate(16, lambdas)))))

ggplot(coeff_df) + geom_line(aes(x = log_lambda, y = coefficient, color = name)) + 
  geom_vline(xintercept=log(lasso_fit_all_feat$cv_results$lambda_best), 
             linetype="dotted") +
  annotate('text', x = -7.8, y = 4, label = "NDAI_smoothed", size = 4) + 
  annotate('text', x = -5.2, y = 1.75, label = "CORR_smoothed", size = 4) + 
  annotate('text', x = -4.8, y = 0.3, label = "DF_smoothed",  size = 4) + 
  theme(legend.position="none") + set_axis_size 
@
\end{subfigure}
\caption{The LASSO regression path; that is, we plot the values of the regression coefficients of the 16 features (eight original, eight smoothed) as a function of the penalty parameter $\lambda$. The vertical dotted line is the optimal $\lambda$ chosen by cross-validation. At this point, we see that there are three clearly non-zero coefficients: smoothed NDAI, smoothed CORR, and smoothed DF. }
\label{fig:lasso_penalty}
\end{figure}

We can also get feature importances from the random forest by looking at the mean decrease in Gini coefficient when a node splits on a feature. Hence, we fit a random forest on the first two images using all 16 features, the eight original plus the eight smoothed. From figure~\ref{fig:rf_varImp}, we see that NDAI and smoothed NDAI are the two most important features. This agrees will with figure ({\color{red} reference figure from EDA that shows NDAI splits the classes well}), and the result from the paper ({\color{red} re-read paper, see what they said about NDAI}). 

\begin{figure}[h]
\centering
\begin{subfigure}{0.6\textwidth}
<<rf_feature_importance, cache = TRUE>>=
# fit rf to all 16 features, on images 1 and 2
# we shall examine feature importances

image12 <- rbind(image1, image2)

labeled_indx <- which(image12$label != 0)
X <- image12[labeled_indx, -c(1,2,3)] # use both smoothed and unsmoothed features
y <- droplevels(image12$label[labeled_indx])

data <- cbind(y, X)
fit_rf_16features <- ranger(y ~ ., 
                    data = data, 
                   num.trees = 10, 
                   probability = TRUE, 
                   importance = 'impurity')
@

<<rf_feature_importance_plot, cache = TRUE, fig.height=7, fig.width=10>>=
# examine feature importances
var_imp <- sort(fit_rf_16features$variable.importance) / 
  sum(fit_rf_16features$variable.importance)
features <- names(var_imp)
names(var_imp) <- c()

var_imp_df <- data.frame(Importance = var_imp, Features = features)
ggplot(var_imp_df, aes(x = reorder(Features, -Importance), y = Importance)) + 
  geom_bar(stat = "identity", fill = 'light blue') + blank_theme + 
  theme(axis.text.x=element_text(angle=90,hjust=1)) + 
  xlab('Features') + set_axis_size

@
\end{subfigure}
\caption{Random forest feature importances as measured by mean decrease in GINI impurity. }
\label{fig:rf_varImp}
\end{figure}


\section{Discussion}
- final model chosen: Show some diagnostic plots or information related to convergence or
parameter estimation. {\color{red} Figure 4 kinda adresses this for LASSO; I also have a plot showing how CV worked in choosing the optimal penalty in LASSO. Maybe this is enough. }

From Figures \ref{fig:map_smo_features} and \ref{fig:map_orig_features}, we notice that in the upper left corner of image 3, there is a region corresponding to expert label +1. (The coordinate \texttt{x} is between 100 and 250, and \texttt{y} is between 200 and 350). All of our 3 algorithms didn't perform well in that region, where they misclassified most of the pixels into -1. 

All of our models were built purely on images 1 and 2, without resorting to image 3 for validation until we produce the report. Assuming that image 3 is representative of future data and independent of image 1 and 2, it would be reasonable to expect a similarly good performance on the forthcoming data sets.

\printbibliography
\end{document}
