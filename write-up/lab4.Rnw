\documentclass{article}

\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{fancyhdr}
\pagestyle{fancy}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{algorithm,algorithmic}
\usepackage{subcaption}
\usepackage{color}
\usepackage{hyperref}
\usepackage{multirow}



\begin{document}

\title{Lab 4 - Cloud Detection}

\maketitle
<<setup, echo = FALSE>>=
# set defaults for future code chunks
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, 
                      dev='png', dpi = 300)
@


<<libraries, cache = FALSE>>=
library(tidyverse)
library(glmnet)
library(knitr)

source('./R/data_processing_utils.R')
source('./R/lasso_utils.R')

@

<<load_data, cache = TRUE>>=
# first two columns are coordinates
# third columns are labels
# next eight are original features
# last eight are smoothed features 
image1 <- prep_image(filename = 'image1.txt', path = '../image_data/')
image2 <- prep_image(filename = 'image2.txt', path = '../image_data/')
image3 <- prep_image(filename = 'image3.txt', path = '../image_data/')
@

\section{Introduction}
\subsection{Source of data (RN)}
- definitions/ data source
- expert labels plot
- models used to predict labels

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
knitr::include_graphics('./panda.jpg')
@
\subcaption{expert labeled image1}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
knitr::include_graphics('./panda.jpg')
@
\subcaption{expert labeled image2}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
knitr::include_graphics('./panda.jpg')
@
\subcaption{expert labeled image1}
\end{subfigure}
\caption{Expertly labeled figures}
\label{fig:expert_labels}
\end{figure}


\subsection{EDA (RN)}
- Differences between angle radiances
- cloud/nocloud angles
- cloud/nocloud 

\section{Models}

\subsection{Feature smoothing}\label{feature_selection}
In image data, neighboring pixels are generally not independent; in particular, there is reason to believe there to be some ``continuity" in the structure of an image. Looking at figure~\ref{fig:expert_labels}, we see that a pixel labeled as a cloud, is usually surrounded by other cloudy pixels; similarly, a pixel labeled as ice is usually surrounded by other icy pixels. 

Hence, similar to how the engineered features NDAI, CORR, and SD in ({\color{red} cite}) at a pixel takes information from its surrouding $8\times 8$ grid of pixels, we also smooth the features. In addition to the original eight features, we add eight more feautres that are smoothed versions of the original: that is, for each pixel and each original feature, we add a feature that is the average of that original feature over its 64 nearest neighbors. 

In our results, we will compare the predictive accuracy of algoirithms that only use the eight original features against those that use the smoothed features. 

\subsection{Logistic regression and LASSO}\label{lasso_descr}
As a first attempt to predict the expert labels, we fit a logistic regression using the eight features, as well as the eight features plus their smoothed their smoothed values. 
However, to guard against overfitting, we then fit a logistic regression to those features described above but with an $L1$ penalty. In doing so, we hope that this regularization will give us better test accuracy by not overfitting to the training set, and it has the added benefit of selecting the important features needed for the prediction. For example, NDAI was noted in the paper  ({\color{red} cite}) and in our plots above ({\color{red} reference}) to be a potentially informative feature. We hope that this LASSO procedure will detect NDAI and other important features. 

\subsubsection*{Model selection and validation}
The regularization parameter was chosen using 2-fold cross-validation: we fit on the first image and validate on the second image; we then flip the procedure and fit to the second image and validate on the first. The predictive accuracy of these two folds are averaged, and we choose the optimal regularization parameter to maximize this averaged predictive accuracy. 

We chose to do 2-fold cross-validation since in image data, each pixel is likely not independent. A given pixel is likely to be similar to its neighbors; for example, from figure~\ref{fig:expert_labels} a cloud pixel is usually surrounded by by other cloudy pixels, and an ice pixel is usually surrounded by other ice pixels. Hence, if we randomly split an image into K-folds, these folds are not independent, and our accuracy measurement will be over-optimstic. Our choice to do 2-fold cross-validation rests on the assumption that while individual pixels are not independent, we assume that the two images are independent. We hope that this independence will give more realistic accuracy measurements. 

\subsection{RF (YN) }

\subsubsection*{Model selection and validation}
describe how CV was done (if at all), and what were your test and training sets. 

\subsection{SVM (RN) }
\subsubsection*{Model selection and validation}
describe how CV was done (if at all), and what were your test and training sets. 



\section{Results}

\subsection{Predictive accuracy}
We present the predictive accuracy of using logistic regression, logistic-LASSO, random forest, and support vector machines. We examine the the difference in prediction accuracy on image2 and image3, and examine the benefits of having smoothed features. Our main results are summarized in table~\ref{tab:acc_results}. 

\subsubsection*{Logistic regression}

<<logistic_reg, cache = TRUE>>=
# fit logistic regression to first image
logistic_fit <- get_logistic_fit(image1, smooth_features = FALSE)
logistic_fit_smo <- get_logistic_fit(image1, smooth_features = TRUE)

# predict on second image
image2_pred <- get_logistic_prediction(logistic_fit, image2)
image2_pred_smo <- get_logistic_prediction(logistic_fit_smo, image2)

# predict on third image
image3_pred <- get_logistic_prediction(logistic_fit, image3)
image3_pred_smo <- get_logistic_prediction(logistic_fit_smo, image3)
@

As a baseline, we use a simple logistic regression on the original eight features to predict the expert labels. We fit the model to the first image, and examine predictive accuracy on the second and third images. With the original features, the predictive accuracy on the second image is \Sexpr{image2_pred$accuracy}, while the predictive accuracy on the third image is \Sexpr{image3_pred$accuracy}. 

We also fit a logistic regression on the smoothed features as described in section~\ref{smoothed_features}. Again fitting to image1, the predictive accuracies on images 2 and 3 are \Sexpr{image2_pred_smo$accuracy} and \Sexpr{image3_pred_smo$accuracy}, respectively. Therefore, it appears that smoothing the features did not have a significant effect when doing logistic regression.  

<<lasso, cache = TRUE, results = 'hide'>>=
lasso_fit <- get_lasso_fit_2fold_cv(image1, image2, smooth_features = FALSE)
lasso_fit_smo <- get_lasso_fit_2fold_cv(image1, image2, smooth_features = TRUE)

image3_lasso_pred <- get_lasso_prediction(lasso_fit$fit, image3)
image3_lasso_smo_pred <- get_lasso_prediction(lasso_fit_smo$fit, image3)
@

\subsubsection*{LASSO}
We now fit the logistic LASSO, which encourages sparsity in the regression coefficients.
 Our goal in doing so is two fold: firstly, we hope that the $L1$ penalty will help avoid overfitting to the training set; and secondly, we hope that the nonzero coefficients found by the LASSO will give insight to the important features needed for cloud detection. 
 
Unlike in the normal logistic regression, we here must do cross-validation to choose the regularization parameter. We do so using 2-fold CV as with image1 as the first fold and image2 as the second fold as described in section~\ref{lasso_descr}. The third image was held out as a test set. 

The predictive accuracy on the held out third image was \Sexpr{image3_lasso_pred$accuracy} when using the original features, and \Sexpr{image3_lasso_smo_pred$accuracy} when using the smoothed features. Like in the logistic regression, smoothing the features did not appear to make a big difference here. 

\subsubsection*{Random Forest}
predictive accuracy on second image vs. third image. 
predictive accuracy on smoothed vs. non-smoothed features. 

\subsubsection*{Support Vector Machine}
predictive accuracy on second image vs. third image. 
predictive accuracy on smoothed vs. non-smoothed features. 


\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
knitr::include_graphics('./panda.jpg')
@
\subcaption{expert labeled image3}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
knitr::include_graphics('./panda.jpg')
@
\subcaption{lasso prediction on image 3}
\end{subfigure}\\
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
knitr::include_graphics('./panda.jpg')
@
\subcaption{SVM prediction on image 3}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
knitr::include_graphics('./panda.jpg')
@
\subcaption{RF prediction on image 3}
\end{subfigure}
\caption{Expertly labeled figures}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{cc|cccc}
 & & \multicolumn{4}{c}{Predictive accuracy} \\
 & & logistic regression & LASSO & RF & SVM \\
    \hline
    \multirow{2}{*}{Image2}& smoothed features & 
    \Sexpr{image2_pred_smo$accuracy} & NA & 100 & 100\\
    & original features & 
    \Sexpr{image2_pred$accuracy} & NA & 100 & 100\\
    \hline
    \hline
    \multirow{2}{*}{Image3}& smoothed features & 
    \Sexpr{image3_pred_smo$accuracy} & \Sexpr{image3_lasso_smo_pred$accuracy} & 100 & 100\\
    &original features & 
    \Sexpr{image3_pred$accuracy} & \Sexpr{image3_lasso_pred$accuracy} & 100 & 100\\
    \hline
\end{tabular}
\caption{accuracy on test set}
\label{tab:acc_results}
\end{table}

\subsection{Feature selection}
Doing LASSO we found blah blah features to be important. 

Doing RF we found blah blah features to be important. 

Hence SVM only used these blah blah features. 

\section{Discussion}
- final model chosen: Show some diagnostic plots or information related to convergence or
parameter estimation.
For your best classification model(s), do you notice any patterns in the misclassification errors? Again,
use quantitative and visual methods of analysis. Do you notice problems in particular regions, or in
specific ranges of feature values?
6. How well do you think your model will work on future data without expert labels?


\end{document}
