\documentclass{article}

\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{fancyhdr}
\pagestyle{fancy}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{algorithm,algorithmic}
\usepackage{subcaption}
\usepackage{color}
\usepackage{hyperref}


\begin{document}

\title{Lab 4 - Cloud Detection}

\maketitle
<<setup, echo = FALSE>>=
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, 
                      dev='png', dpi = 300)
@


\section{Introduction}
\subsection{Source of data (RN)}
- definitions/ data source
- expert labels plot
- models used to predict labels

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
knitr::include_graphics('./panda.jpg')
@
\subcaption{expert labeled image1}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
knitr::include_graphics('./panda.jpg')
@
\subcaption{expert labeled image2}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
knitr::include_graphics('./panda.jpg')
@
\subcaption{expert labeled image1}
\end{subfigure}
\caption{Expertly labeled figures}
\end{figure}

\subsection{EDA (RN)}
- Differences between angle radiances
- cloud/nocloud angles
- cloud/nocloud 

\section{Models}
\subsetion

\subsection{Logistic regression and LASSO}


\subsection{RF (YN) }

\subsection{SVM (RN) }

\subsection{Model selection and validation}
To validate the performance of the algorithms above, we held out the third image as a test set, un-opened until our models were set. Algorithms were trained on the first two images only. In particular, model selection was done using 2-fold cross validation: we fit on the first image and validate on the second image; we then flip the procedure and fit to the second image and validate on the first. The results (usually predictive accuracy) of these two folds are averaged, and we choose the optimal model parameter to maximize this averaged predictive accuracy. 

\section{Results}
\subsection{Accuracy}

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
knitr::include_graphics('./panda.jpg')
@
\subcaption{expert labeled image3}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
knitr::include_graphics('./panda.jpg')
@
\subcaption{lasso prediction on image 3}
\end{subfigure}\\
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
knitr::include_graphics('./panda.jpg')
@
\subcaption{SVM prediction on image 3}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
<<>>=
knitr::include_graphics('./panda.jpg')
@
\subcaption{RF prediction on image 3}
\end{subfigure}
\caption{Expertly labeled figures}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{lllll}
& logistic & LASSO & RF & SVM \\
smoothed features & 100\% !! & 100\% !! & 100\% !! & 100\% !!\\
original features & 99\% !! & 99\% !! & 99\% !! & 99\% !!
\end{tabular}
\caption{accuracy on test set}
\end{table}

\subsection{Features}
- quantitative and visual justification

\section{Discussion}
- final model chosen: Show some diagnostic plots or information related to convergence or
parameter estimation.
For your best classification model(s), do you notice any patterns in the misclassification errors? Again,
use quantitative and visual methods of analysis. Do you notice problems in particular regions, or in
specific ranges of feature values?
6. How well do you think your model will work on future data without expert labels?


\end{document}
