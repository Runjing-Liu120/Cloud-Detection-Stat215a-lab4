\documentclass{article}

\usepackage{geometry}
\usepackage[backend=bibtex]{biblatex}
\bibliography{references}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{fancyhdr}
\pagestyle{fancy}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{algorithm,algorithmic}
\usepackage{subcaption}
\usepackage{color}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{float}
\usepackage{wrapfig}


\begin{document}

\title{Lab 4 - Cloud Detection}

\maketitle
<<setup, echo = FALSE>>=
# set defaults for future code chunks
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, 
                      dev='png', dpi = 300)
@


<<libraries, cache = FALSE>>=
library(tidyverse)
library(glmnet)
library(knitr)
library(ggplot2)
library(dplyr)
library(reshape2)
library(GGally)

source('./R/data_processing_utils.R')
source('./R/lasso_utils.R')
source('./R/rf.R')
source('./R/SVM_utils.R')

blank_theme <- theme_minimal(base_size = 30) +
  theme(plot.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank()) + 
  theme(axis.line.x = element_line(colour = 'black', size=0.5, linetype='solid'))

blank_theme_small <- theme_minimal(base_size = 20) +
  theme(plot.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank()) + 
  theme(axis.line.x = element_line(colour = 'black', size=0.5, linetype='solid'))

blank_theme_large <- theme_minimal(base_size = 45) +
  theme(plot.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank()) + 
  theme(axis.line.x = element_line(colour = 'black', size=0.5, linetype='solid'))

remove_y <-   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())

@

<<axis_sizes, cache = TRUE>>=
title_size <- 25
axis_title_size <- 20
axis_element_size <- 20

set_axis_size <- theme(axis.text=element_text(size=axis_element_size), 
                       axis.title = element_text(size=axis_title_size), 
                       plot.title = element_text(size=title_size))

@


<<load_data, cache = TRUE>>=
# first two columns are coordinates
# third columns are labels
# next eight are original features
# last eight are smoothed features 

path = '../image_data/'

image1.plots <- load_image(filename = 'image1.txt', path)
image2.plots <- load_image(filename = 'image2.txt', path)
image3.plots <- load_image(filename = 'image3.txt', path)

image1 <- prep_image(image1.plots)
image2 <- prep_image(image2.plots)
image3 <- prep_image(image3.plots)

ones <- image1.plots %>%
  filter(label == 1)

negative_ones <- image1.plots %>%
  filter(label == -1)

set.seed(19)
s <- sample(115229, 10000)

@

\section{Introduction}

In this report, we investigate the application of various statistical and machine learning models for a task in image recognition. Specifically, we apply logistic regression, LASSO, random forest, and support vector machines in distinguishing between pixels representing either cloud or ice cover from satellite imagery. We build on the work presented in {\color{red} cite, name the paper}, which utilizes thresholding on three features of the imagery through an enhanced linear correlation matching (ELCM) algorithm. The images used to train and test our models are from the National Aeronautics and Space Administration's (NASA) Multiangle Imaging SpectroRadiometer (MISR). In light of the climate change-driven melting of the ice caps in the polar regions, it becomes increasingly important and difficult to monitor ice cover remotely, provided feedback effects increasing the frequency of cloud cover. We begin our report with an exploration and analysis of three MISR images and their features.



\subsection{Image Properties}



For each pixel within each image dataframe, the following information is included: the x and y placement within the image, an expert labeling of either 1, 0, or -1 (cloud, unclear, and no cloud, respectively), red-band radiance measurements from five different angles, the correlation of MISR images of the same scene from different MISR viewing directions (CORR), the standard deviation of the MISR nadir camera pixel values accross a scene (SD), and the normalized difference angular index (NDAI). We train and validate our models with the expert labels, shown for each image below:

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.3\textwidth}
<<image_plot1, cache = TRUE>>=
plotr(image1.plots, image1.plots$label, "none")
@
\subcaption{expert labeled image 1}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
<<image_plot2, cache = TRUE>>=
plotr(image2.plots, image2.plots$label, "none") +
  remove_y
@
\subcaption{expert labeled image 2}
\end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
<<image_plot3, cache = TRUE>>=
plotr(image3.plots, image3.plots$label, "right") +
  remove_y
@
\subcaption{expert labeled image 3}
\end{subfigure}
\caption{Expert labels of all three images. -1 = no cloud, 0 = unclear, 1 = cloud}
\label{fig:expert_labels}
\end{figure}

As we can see in Figure 1, the labels for each image present an amorphous grouping of clouds over the polar regions. Additionally, as seen by the green regions separating the blue and the red regions in each image, the expert seemed most uncertain at the thresholds between clouds and ice cover in distinguishing between clouds and ice. For the prediction of these labels, we consider the five different radiance measurements, CORR, SD, and NDAI as features for our model. Whether these groupings are realized by the features, and how the features relate to one another based on these labels, is explored next.

\subsubsection{Angle radiance measures}

\begin{wrapfigure}{r}{0.6\textwidth}
  \vspace{-50pt}
  \begin{center}
<<image_plot100, cache = TRUE, fig.align='center', out.width='0.6\\textwidth'>>=
suppressMessages(require(ggplot2))
ggpairs(ones[s, 7:11], lower=list(continuous = wrap("smooth", alpha = 0.2, fill = NA)),
        diag=list(continuous="barDiag"),
        upper = list(corSize=20)) + blank_theme_small
@
  \end{center}
  \vspace{-20pt}
  \caption{Correlations between radiances captured at angles DF, CF, BF, AF, and AN for expert labels = 1, subsampled to 10,000}
  \vspace{-5pt}
\end{wrapfigure}

For each image, we are given radiance measures of the red-band from cameras recording at various angles, named as follows: DF, CF, BF, AF, and AN. The most forward-viewing oblique camera is DF and the most aft-viewing oblique camera is AN, with the progression between the two being represented by the degression of the alphabetical labeling. The radiance value distributions and correlations between the measurements taken at these different angles, for the pixels expertly labeled 1, are summarized in the figure to the right. Here we see that this change in obliqueness correlates with the correlation between the different angles. Correlation values are much higher between angles that are closer to one-another than those that are far apart. This can be attributed to the change in the positioning of the clouds as each angle measure is taken. This phenomena is addressed by {\color{red} cite, name the paper}. While the ground pixels recordings remain rather consitent as the satellite travels over the region, the clouds both naturally shift and are obscured by the angle of measurement. Additionally, in figure 2 we see the distribution of the radiance measures transition to a bimodal distribution as we shift from the forward-viewing oblique camera to the most aft-viewing oblique camera (from DF to AN). As can be seen in figure 3 below, this shift to bimodality is not seen in the distribtutions of the radiance values of the other pixels (those not expertly labeled as -1). 

\begin{figure}[H]
\centering
\begin{subfigure}[t]{0.22\textwidth}
<<image_plot5, cache = FALSE>>=
ggplot(image1.plots) + 
  geom_density(aes(x = DF, group = label, fill = label), 
               alpha = 0.5) +
  blank_theme_large +
  theme(legend.position="none")
@
\end{subfigure}
\begin{subfigure}[t]{0.22\textwidth}
<<image_plot6, cache = FALSE>>=
ggplot(image1.plots) + 
  geom_density(aes(x = CF, group = label, fill = label), 
               alpha = 0.5) +
  blank_theme_large +
  theme(legend.position="none",
        axis.title.y=element_blank())
@
\end{subfigure}
\begin{subfigure}[t]{0.22\textwidth}
<<image_plot7, cache = FALSE>>=
ggplot(image1.plots) + 
  geom_density(aes(x = BF, group = label, fill = label), 
               alpha = 0.5)  +
  blank_theme_large +
  theme(legend.position="none",
        axis.title.y=element_blank())
@
\end{subfigure}

\begin{subfigure}[t]{0.22\textwidth}
<<image_plot8, cache = FALSE>>=
ggplot(image1.plots) + 
  geom_density(aes(x = AF, group = label, fill = label), 
               alpha = 0.5)  +
  blank_theme_large +
  theme(axis.title.y=element_blank(),
        legend.position="none")
@
\end{subfigure}
\begin{subfigure}[t]{0.22\textwidth}
<<image_plot9, cache = FALSE>>=
ggplot(image1.plots) + 
  geom_density(aes(x = AN, group = label, fill = label), 
               alpha = 0.5)  +
  blank_theme_large +
  guides(color=guide_legend("Label",
    override.aes = list(size=10))) +
  theme(axis.title.y=element_blank())
@
\end{subfigure}
\caption{Radiance value distributions for each angle subset by expert label for image 1}
\label{fig:cloud/nocloud_rad_image1}
\end{figure}


\subsubsection{NDAI, CORR, and SD measures}

As previously stated, NDAI, CORR, and SD are measurements of the normalized difference angular index, the correlation of MISR images of the same scene from different MISR viewing directions, and the standard deviation of the MISR nadir camera pixel values accross a scene, respectively. We observe their distributions based on expert label, for image 1 and 2, in figures 3 and 4 below.

\begin{figure}[H]
\centering
\begin{subfigure}[t]{0.22\textwidth}
<<image_plot10, cache = FALSE>>=
ggplot(image1.plots) + 
  geom_density(aes(x = NDAI, group = label, fill = label), 
               alpha = 0.5) +
  blank_theme_large +
  theme(legend.position="none")
@
\subcaption{}
\end{subfigure}
\begin{subfigure}[t]{0.22\textwidth}
<<image_plot11, cache = FALSE>>=
ggplot(image1.plots) + 
  geom_density(aes(x = CORR, group = label, fill = label), 
               alpha = 0.5) +
  blank_theme_large +
  theme(legend.position="none",
        axis.title.y=element_blank())
@
\subcaption{}
\end{subfigure}
\begin{subfigure}[t]{0.22\textwidth}
<<image_plot12, cache = FALSE>>=
ggplot(image1.plots) + 
  geom_density(aes(x = SD, group = label, fill = label), 
               alpha = 0.5)  +
  blank_theme_large +
  guides(color=guide_legend("Label",
    override.aes = list(size=20))) +
  theme(axis.title.y=element_blank()) +
  scale_x_continuous(breaks = seq(0, 4))
@
\subcaption{}
\end{subfigure}
\caption{SD, NDAI, AND CORR distributions for each angle subset by expert label for image 1}
\label{fig:cloud/nocloud_image1}

\end{figure}

As seen by the separated red and blue peaks in figure 3a and 4a, NDAI seems to distinguish well between cloud and no-cloud pixels for image 1 and 2. Additionally, from these two plots we also see that NDAI values tend to categorize the pixels where the expert was uncertain with the cloud-labeled pixels. As seen by the remaining plots in figures 3 and 4, CORR and SD are not as consistent with their categorization between the cloud and no-cloud pixels between image 1 and 2. While figure 3b shows very similar distributions for all three labels for image 1's CORR values, figure 4b shows pixels labeled 1 and -1 to have two distinct distribution for image 2's CORR values. This means that it may not be best to use CORR as one of the predicting features, since it is inconsistent between images.

\begin{figure}[H]
\centering
\begin{subfigure}[t]{0.22\textwidth}
<<image_plot13, cache = FALSE>>=
ggplot(image2.plots) + 
  geom_density(aes(x = NDAI, group = label, fill = label), 
               alpha = 0.5) +
  blank_theme_large +
  theme(legend.position="none")
@
\subcaption{}
\end{subfigure}
\begin{subfigure}[t]{0.22\textwidth}
<<image_plot14, cache = FALSE>>=
ggplot(image2.plots) + 
  geom_density(aes(x = CORR, group = label, fill = label), 
               alpha = 0.5) +
  blank_theme_large +
  theme(legend.position="none",
        axis.title.y=element_blank())
@
\subcaption{}
\end{subfigure}
\begin{subfigure}[t]{0.22\textwidth}
<<image_plot15, cache = FALSE>>=
ggplot(image2.plots) + 
  geom_density(aes(x = SD, group = label, fill = label), 
               alpha = 0.5)  +
  blank_theme_large +
  guides(color=guide_legend("Label",
    override.aes = list(size=10))) +
  theme(axis.title.y=element_blank()) +
  scale_x_continuous(breaks = seq(0, 4))
@
\subcaption{}
\end{subfigure}
\caption{SD, NDAI, and CORR distributions for each angle subset by expert label for image 2}
\label{fig:cloud/nocloud_image2}
\end{figure}


\subsection{Feature smoothing}\label{feature_smoothing}
In our analysis, we will consider not only the eight features, but also smoothed versions of them. We do this because in image data, neighboring pixels are generally not independent; in particular, there is usually be some ``continuity" in the structure of an image. Looking at figure~\ref{fig:expert_labels}, we see that a pixel labeled as a cloud is usually surrounded by other cloudy pixels; similarly, a pixel labeled as ice is usually surrounded by other icy pixels. 

Hence, inspired by how the engineered features NDAI, CORR, and SD take radiances from a surrouding $8\times 8$ grid of pixels, we also take in information from neighboring pixels. Instead of considering the original features, we consider features that are smoothed versions of the original: that is, for each pixel and each original feature, we add a feature that is the average of that original feature over its 64 nearest neighbors. 

We also hope that this smoothing may give better test accuracy by ameliorating the effect of outliers that may exist in the dataset. 

In our results, we will compare the predictive accuracy of algorithms that use the eight original features against those that use the smoothed features. 


\section{Models}
We describe here the four algorithms used: logistic regression, logistic LASSO, random forest, and support vector machines. We only consider binary classification, with the two classes being cloud or ice; we ignore the unlabeled pixels where the expert was uncertain. 

\subsection{Logistic regression and LASSO}\label{lasso_descr}
As a first attempt to predict the expert labels, we fit a logistic regression using the first two images as our training set. 

However, to guard against overfitting, we then tried the logistic LASSO; that is, we run a logistic regression with an $L1$ penalty. The strength of this penalty was chosen by 2-fold cross validation using the first two images, as described in section~\ref{CV_descr} below. By doing LASSO, we hope that the regularization will give us better test accuracy by not overfitting to the training set, and the sparsity has the added benefit of suggesting which features are necessary for the prediction. For example, NDAI was noted in the paper \cite{shi2008daytime} and in our plots above (figure~\ref{fig:cloud/nocloud_image1}a and \ref{fig:cloud/nocloud_image2}a) to be a feature that well separates the two classes. We hope that this LASSO procedure will detect NDAI and other important features. 


\subsection{RF}\label{rf_descr}
Since the paper \cite{shi2008daytime} made its predictions by thresholding the features, it seemed natural to see if a random forest could make better predictions, since a random forest essentially uses multiple layers of thresholding to partition the feature space. 

Our random forest was trained on the first two images. At each node, 2 features were tested to determine the split and trees were grown to purity. The forest consisted of 500 trees. 

% We make the trees to return probabilities of the predicted labels instead of binary responses, and then apply thresholding (as we did in logisitic regression) to get the predicted labels.
% 
% \subsubsection*{Model selection and validation}
% {\color{red} remove this section, we'll have a general model selection and validation section below. }
% 
% The model was first trained on image 1, and then the model was tested on image2 and image 3. The 2-fold cross validation was performed in that we also trained the model on image 3, and tested on image 1 and image 2. We use Random Forest as an out-of-the-box algorithm so there are no parameters to be tuned. 

\subsection{SVM}
Finally, we also used support vector machines for classification. However, we only gave the SVM three features: NDAI, CORR, and DF, since these were features found to be important by the LASSO (see section~\ref{feature_selection}). The cost for constraint violation was chosen with 2-fold cross-validation as described in section~\ref{CV_descr}. 

\subsection{Model selection and validation}\label{CV_descr}
We used the first two images as our training set, and hold out the third image as a test set. To measure prediction accuracies, we only use the pixels with expert labels, ignoring the unlabeled pixels (since we are doing binary classification).

When parameters needed to be tuned, we used 2-fold cross-validation the first two images (our training set) with an entire image as one fold. In other words, we fit on the first image and validate on the second image; we then flip the procedure and fit to the second image and validate on the first. The predictive accuracy of these two folds are averaged, and we choose the parameter to maximize this averaged predictive accuracy. 

For SVM, we used this procedure to find the cost parameter for constraint violation; for LASSO, we used CV to find for the the strength of the L1 penalty. 

We chose to do 2-fold cross-validation since in image data, each pixel is likely not independent. A given pixel is likely to be similar to its neighbors; for example, from figure~\ref{fig:expert_labels} a cloud pixel is usually surrounded by by other cloudy pixels, and an ice pixel is usually surrounded by other ice pixels. Hence, if we randomly split an image into K-folds, these folds are not independent, and our accuracy measurement will be over-optimstic. Our choice to do 2-fold cross-validation rests on the assumption that while individual pixels are not independent, it may be more realistic to believe that the two images are independent. We hope that this independence will give more conservative training error measurements, and help avoid overfitting. In the end, the success of our procedures will be evaluated by predicting on the held out third image.  

\section{Results}

\subsection{Predictive accuracy}
We present the predictive accuracy of using logistic regression, logistic-LASSO, random forest, and support vector machines, and compare the benefits, if any, of using the smoothed features instead of the original features. Since we are only doing binary classification, accuracy is measured by the success of predicting the expert labeled pixels; we ignore the unlabeled pixels. Our predictive accuracies are summarized in table~\ref{tab:acc_results}, while figures~\ref{fig:map_orig_features} and \ref{fig:map_smo_features} visually compares the expert labels with the predicted labels. 

As a comparison, the training accuracy on image 1 is reported in the table \ref{tab:training_accu_results}; naturally, the training accuracy significantly over-estimatates the testing accuracy. 

\subsubsection*{Logistic regression}

<<logistic_reg, cache = TRUE>>=

# fit logistic regression to first two images
image12 <- rbind(image1, image2)
logistic_fit <- get_logistic_fit(image12, smooth_features = FALSE)
logistic_fit_smo <- get_logistic_fit(image12, smooth_features = TRUE)

# predict on third image
image3_pred <- get_logistic_prediction(logistic_fit, image3.plots)
image3_pred_smo <- get_logistic_prediction(logistic_fit_smo, image3.plots)
@

As a baseline, we fit a simple logistic regression. We train the model using the first and second images, and examine the predictive accuracy on the third image. Using the original features, our predictive accuracy is \Sexpr{image3_pred$accuracy}. Using the smoothed features, we get an accuracy of \Sexpr{image3_pred_smo$accuracy}. Therefore, it appears that smoothing the features did not have a significant effect when doing logistic regression.  

\subsubsection*{LASSO}

<<lasso, cache = TRUE, results = 'hide'>>=
# fit lasso using 2-fold CV on first two images
lasso_fit <- get_lasso_fit_2fold_cv(image1, image2, smooth_features = FALSE)
lasso_fit_smo <- get_lasso_fit_2fold_cv(image1, image2, smooth_features = TRUE)

# predict on third image
image3_lasso_pred <- get_lasso_prediction(lasso_fit$fit, image3.plots)
image3_lasso_smo_pred <- get_lasso_prediction(lasso_fit_smo$fit, image3.plots)
@

We now examine the success of the LASSO, trained on the first two images with cross validation as described in section~\ref{CV_descr}. The predictive accuracy on the held out third image was \Sexpr{image3_lasso_pred$accuracy} when using the original features, and \Sexpr{image3_lasso_smo_pred$accuracy} when using the smoothed features. Like in the logistic regression, smoothing the features did not appear to make a big difference here. 

{\bf Diagnostic plots for LASSO}: 
As discussed in section~\ref{CV_descr}, we do 2-fold cross validation to choose the optimal regularization parameter: we fit on image1 and predict on image2, then fit on image2 and predict on image1. The two predictive accuracies are averaged, and we choose the lambda with the highest predictive accuracy. The results of this cross validation is shown in figure~\ref{fig:lasso_cv}. 

Moreover, we also examine the coefficient paths in fitting the LASSO. This is shown in figure~\ref{fig:lasso_penalty}, and discussed in more detail when we consider feature selection below (section~\ref{feature_selection}). 

\begin{figure}[h]
\centering
\begin{subfigure}{0.3\textwidth}
<<lasso_cv, cache = TRUE, fig.height = 4, fig.width = 6>>=
ggplot() + geom_point(aes(x = log(lasso_fit$cv_results$lambda), 
                          y = lasso_fit$cv_results$accuracy), 
                      color = 'blue', size = 0.5) + 
  ylab('CV classification accuracy') + xlab('log lambda') +
  geom_vline(xintercept=log(lasso_fit$cv_results$lambda_best)) + set_axis_size
@
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
<<lasso_smo_cv, cache = TRUE, fig.height = 4, fig.width = 6>>=
ggplot() + geom_point(aes(x = log(lasso_fit_smo$cv_results$lambda), 
                          y = lasso_fit_smo$cv_results$accuracy),
                      color = 'blue', size = 0.5) + 
  ylab('CV classification accuracy') + xlab('log lambda') +
  geom_vline(xintercept=log(lasso_fit_smo$cv_results$lambda_best)) + set_axis_size
@
\end{subfigure}
\caption{Cross-validated accuracy as a function of the regularaization parameter $\lambda$ The vertial line indicates the optimal $\lambda$. (a) fitting with the original eight features; (b) fitting on the eight smoothed features}
\label{fig:lasso_cv}
\end{figure}





\subsubsection*{Random Forest}

<<rf, cache = TRUE, results = 'hide'>>=
# image1_neighbor <- add_neighbor_info(image1)
# image2_neighbor <- add_neighbor_info(image2)
# image3_neighbor <- add_neighbor_info(image3)

# train on images 1 and 2
rf_fit_orig <- get_rf_fit(image12, smooth_features = FALSE)
rf_fit_orig_smo <- get_rf_fit(image12, smooth_features = TRUE)
@

<<rf_pred, cache = TRUE, results = 'hide'>>=
# predict on image 3

image3_rf_pred <- get_rf_prediction(rf_fit_orig, image3.plots)
image3_rf_smo_pred <- get_rf_prediction(rf_fit_orig_smo, image3.plots)
@

After training the random forest on the first two images with the original features, the predictive accuracy on image 3 was \Sexpr{image3_rf_pred$accuracy}. With feature smoothing, the predictive accuracy on image 3 was \Sexpr{image3_rf_smo_pred$accuracy}. Smoothing may have helped slightly here. 

%Random forest models also reports feature importance by giving the Gini coefficient. In our original model which is trained on image 1 without smoothing, the top 3 most important features are \texttt{NDAI, AF} and \texttt{BF}. If we add the smoothing features, then 3 of the most important features would become \texttt{NDAI\_smoothed}, \texttt{AF\_smoothed} and \texttt{SD\_smoothed}.

%Another group of experiments were conducted on a new dataset where each pixel would have access to the features of its neighboring pixels, i.e., the pixels to the up, down, left and right directions. If we train the model on image 1, then the top 3 of the most important features are \texttt{NDAI, NDAI\_u} and \texttt{NDAI\_l}, which suggests that the neighboring pixels provide more information than its local features.

\subsubsection*{Support Vector Machine}

<<svm_fit, cache = TRUE, results = 'hide'>>=
# warning: this block takes awhile

# fit SVM using 2-fold CV on first two images
svm_fit <- get_svm_fit_2fold_cv(image1, image2, smooth_features = FALSE)
@

<<svm_fit_smo, cache = TRUE, results = 'hide'>>=
# warning: this block takes awhile

# fit SVM using 2-fold CV on first two images
# use smooth features
svm_fit_smo <- get_svm_fit_2fold_cv(image1, image2, smooth_features = TRUE)
@

<<svm_pred, cache = TRUE>>=
# why the fuck does 'predict' take so long too 

# predict on third image
image3_svm_pred <- get_svm_prediction(svm_fit$fit, image3.plots)
image3_svm_smo_pred <- get_svm_prediction(svm_fit_smo$fit, image3.plots)
@

Finally, we examine prediction using an SVM. As described in section \ref{CV_descr}, the first two images were used for training, and 2-fold cross-validation was used to set the cost of contraint violation. 

Here, we only fit using 3 features: NDAI, CORR, and DF, variables found to be important by the LASSO (see section~\ref{feature_selection}). Using these tree features, the predictive accuacy on the held out third image was \Sexpr{image3_svm_pred$accuracy}; after smoothing these three features, the predictive accuracy was \Sexpr{image3_svm_smo_pred$accuracy}.

<<training_accuracy, cache = TRUE, results = 'hide'>>=

# predict on the 1st image
# SVM
image1_svm_pred <- get_svm_prediction(svm_fit$fit, image1.plots)
image1_svm_smo_pred <- get_svm_prediction(svm_fit_smo$fit, image1.plots)
# RF
image1_rf_pred <- get_rf_prediction(rf_fit_orig, image1.plots)
image1_rf_smo_pred <- get_rf_prediction(rf_fit_orig_smo, image1.plots)
# LASSO
image1_lasso_pred <- get_lasso_prediction(lasso_fit$fit, image1.plots)
image1_lasso_smo_pred <- get_lasso_prediction(lasso_fit_smo$fit, image1.plots)
# Logistic
image1_pred <- get_logistic_prediction(logistic_fit, image1.plots)
image1_pred_smo <- get_logistic_prediction(logistic_fit_smo, image1.plots)
@

\begin{table}[tb]
 \centering
 \begin{tabular}{l|llll}
 & logistic & LASSO & RF & SVM \\\hline
 smoothed features & \Sexpr{image1_pred_smo$accuracy} &
 \Sexpr{image1_lasso_smo_pred$accuracy}& \Sexpr{image1_rf_smo_pred$accuracy} & \Sexpr{image1_svm_smo_pred$accuracy}\\
 original features & \Sexpr{image1_pred$accuracy} &  \Sexpr{image1_lasso_pred$accuracy} & \Sexpr{image1_rf_pred$accuracy}& \Sexpr{image1_svm_pred$accuracy}
 \end{tabular}
\caption{Accuracy on image 1, an image that was used for training}
\label{tab:training_accu_results}
\end{table}

\begin{table}[tb]
 \centering
 \begin{tabular}{l|llll}
 & logistic & LASSO & RF & SVM \\\hline
 smoothed features & \Sexpr{image3_pred_smo$accuracy} &
 \Sexpr{image3_lasso_smo_pred$accuracy}& \Sexpr{image3_rf_smo_pred$accuracy} & \Sexpr{image3_svm_smo_pred$accuracy}\\
 original features & \Sexpr{image3_pred$accuracy} &  \Sexpr{image3_lasso_pred$accuracy} & \Sexpr{image3_rf_pred$accuracy}& \Sexpr{image3_svm_pred$accuracy}
 \end{tabular}
\caption{Accuracy on image 3, the image held out for testing. }
\label{tab:acc_results}
\end{table}

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.245\textwidth}
<<cache = TRUE>>=
plotr(image3.plots, image3.plots$label, "none") +
  remove_y + ggtitle('Truth')
@
\subcaption{}
\end{subfigure}
\begin{subfigure}[t]{0.245\textwidth}
<<cache = TRUE>>=
plotr(image3.plots, image3_lasso_pred$pred_class, "none") +
  remove_y + ggtitle('LASSO')
@
\subcaption{}
\end{subfigure}
\begin{subfigure}[t]{0.245\textwidth}
<<cache = TRUE>>=
plotr(image3.plots, image3_rf_pred$pred_class, "none") +
  remove_y + ggtitle('RF')
@
\subcaption{}
\end{subfigure}
\begin{subfigure}[t]{0.245\textwidth}
<<cache = TRUE>>=
plotr(image3.plots, image3_svm_pred$pred_class, "none") +
  remove_y + ggtitle('SVM')
@
\end{subfigure}
\caption{Labels on image 3, using the original eight features. Fits were done with logistic-LASSO, random forest, and support vector machines. }\label{fig:map_orig_features}
\end{figure}

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.245\textwidth}
<<cache = TRUE>>=
plotr(image3.plots, image3.plots$label, "none") +
  remove_y + ggtitle('Truth')
@
\subcaption{}
\end{subfigure}
\begin{subfigure}[t]{0.245\textwidth}
<<cache = TRUE>>=
plotr(image3.plots, image3_lasso_smo_pred$pred_class, "none") +
  remove_y + ggtitle('LASSO')
@
\subcaption{}
\end{subfigure}
\begin{subfigure}[t]{0.245\textwidth}
<<cache = FALSE>>=
plotr(image3.plots, image3_rf_smo_pred$pred_class, "none") +
  remove_y + ggtitle('RF')
@
\subcaption{}
\end{subfigure}
\begin{subfigure}[t]{0.245\textwidth}
<<cache = TRUE>>=
plotr(image3.plots, image3_svm_smo_pred$pred_class, "none") +
  remove_y + ggtitle('SVM')
@
\subcaption{}
\end{subfigure}
\caption{Labels on image 3, using smoothed features. Fits were done with logistic-LASSO, random forest, and support vector machines. }\label{fig:map_smo_features}
\end{figure}

\subsection{Feature selection}\label{feature_selection}

<<lasso_feat_selection, cache = TRUE, results = 'hide'>>=
# fit lasso on all 16 features
# we shall examine the sparisity in the next chunk

image12 <- rbind(image1, image2)

labeled_indx <- which(image12$label != 0)
X <- image12[labeled_indx, -c(1,2,3)] # use both smoothed and unsmoothed features
y <- droplevels(image12$label[labeled_indx])

fit_lasso12_path <- glmnet(as.matrix(X), y, family = "binomial")

lasso_fit_all_feat <- get_lasso_fit_2fold_cv(image1, image2, smooth_features = NULL)
@
The $L1$ penalty in the LASSO not only helps prevent overfitting, but the sparsity pattern also gives insight into which features are truly important for predicting cloud coverage. With this goal in mind, we fit the LASSO on the first two images with 16 features, the original 8 plus their smoothed versions, and see which regression coefficients are nonzero. The coefficients as a function of the penalty parameter is shown in figure~\ref{fig:lasso_penalty}. We choose the penalty parameter using 2-fold cross-validation with the two images as folds (see section~\ref{lasso_descr}). At this optimal lambda, we see that only four features are active: smoothed NDAI, smoothed CORR, smoothed DF, and non-smoothed CORR, with the non-smoothed CORR coefficient being barely non-zero. Hence, this suggests that NDAI, CORR, and DF are the three most important features, and moreover, looking at the smoothed versions may be sufficient. 

\begin{figure}[h]
\centering
\begin{subfigure}{0.6\textwidth}
<<lasso_path, cache = TRUE, fig.height=5, fig.width=6.5>>=
# get lasso fit path

coeff <- as.matrix(fit_lasso12_path$beta)
feature_names <- rownames(coeff)

coeff_vec <- as.vector(coeff)
lambdas <- fit_lasso12_path$lambda

coeff_df <- data.frame(coefficient = coeff_vec, 
                       name = rep(feature_names, 100), 
                       log_lambda = log(as.vector(t(replicate(16, lambdas)))))

ggplot(coeff_df) + geom_line(aes(x = log_lambda, y = coefficient, color = name)) + 
  geom_vline(xintercept=log(lasso_fit_all_feat$cv_results$lambda_best), 
             linetype="dotted") +
  annotate('text', x = -7.8, y = 4, label = "NDAI_smoothed", size = 4) + 
  annotate('text', x = -5.2, y = 1.75, label = "CORR_smoothed", size = 4) + 
  annotate('text', x = -4.8, y = 0.3, label = "DF_smoothed",  size = 4) + 
  theme(legend.position="none") + set_axis_size 
@
\end{subfigure}
\caption{The LASSO regression path; that is, we plot the values of the regression coefficients of the 16 features (eight original, eight smoothed) as a function of the penalty parameter $\lambda$. The vertical dotted line is the optimal $\lambda$ chosen by cross-validation. At this point, we see that there are three clearly non-zero coefficients: smoothed NDAI, smoothed CORR, and smoothed DF. }
\label{fig:lasso_penalty}
\end{figure}

We can also get feature importances from the random forest by looking at the mean decrease in Gini coefficient when a node splits on a feature. Hence, we fit a random forest on the first two images using all 16 features, the eight original plus the eight smoothed. From figure~\ref{fig:rf_varImp}, we see that NDAI and smoothed NDAI are the two most important features. This agrees will with figure~\ref{fig:cloud/nocloud_image1}a and the conclusion from the Tau et al.\cite{shi2008daytime} that NDAI well separates the two classes.

\begin{figure}[h]
\centering
\begin{subfigure}{0.6\textwidth}
<<rf_feature_importance, cache = TRUE>>=
# fit rf to all 16 features, on images 1 and 2
# we shall examine feature importances

image12 <- rbind(image1, image2)

labeled_indx <- which(image12$label != 0)
X <- image12[labeled_indx, -c(1,2,3)] # use both smoothed and unsmoothed features
y <- droplevels(image12$label[labeled_indx])

data <- cbind(y, X)
fit_rf_16features <- ranger(y ~ ., 
                    data = data, 
                   num.trees = 10, 
                   probability = TRUE, 
                   importance = 'impurity')
@

<<rf_feature_importance_plot, cache = TRUE, fig.height=7, fig.width=10>>=
# examine feature importances
var_imp <- sort(fit_rf_16features$variable.importance) / 
  sum(fit_rf_16features$variable.importance)
features <- names(var_imp)
names(var_imp) <- c()

var_imp_df <- data.frame(Importance = var_imp, Features = features)
ggplot(var_imp_df, aes(x = reorder(Features, -Importance), y = Importance)) + 
  geom_bar(stat = "identity", fill = 'light blue') + blank_theme + 
  theme(axis.text.x=element_text(angle=90,hjust=1)) + 
  xlab('Features') + set_axis_size

@
\end{subfigure}
\caption{Random forest feature importances as measured by mean decrease in GINI impurity. }
\label{fig:rf_varImp}
\end{figure}


\section{Discussion}


<<class_accuracies, cache = TRUE>>=
training_class1_propn <- mean(image12$label == 1)
training_class_m1_propn <- mean(image12$label == -1)


labeled_indx_1 <- which(image3$label == 1)
labeled_indx_m1 <- which(image3$label == -1)

labeled_indx <- which(image3.plots$label != 0)
image3_pred <- image3_pred_smo$pred_class[labeled_indx]

acc_on_1 <- mean(image3_pred[labeled_indx_1] == 1)
acc_on_m1 <- mean(image3_pred[labeled_indx_m1] == -1)

# image3 %>% filter(label == 1) %>%
#   ggplot() + 
#     geom_density(aes(x = CORR, 
#                      group = (label == image3_pred[labeled_indx_1]), 
#                      fill = (label == image3_pred[labeled_indx_1])), 
#                  alpha = 0.5) +
#     blank_theme +
#     theme(legend.position="none")

@

In this lab, we examine using logistic regression, logistic-LASSO, random forest, and support vector machines to classify pixels in a satilite image as a cloud or as ice. We trained these algorithms on two images, using cross-validation when necessary, and evaluated them on a held out third image. 

In our results, all three algorithms behaved similarly in terms of test accuracy. Our baseline, the logistic regression, obtained about 75\% predictive accuracy on the test set, while random forest did the best at about 84\% accuracy. It did not seem that using smoothed features aided the predictive accuracy; but it is interesting that using them did not hurt either, despite losing local information in the smoothing. 

We also notice that prediction was more accurate for ice than for clouds; for example, the random forest got 88\% of the ice pixels correct in image3, but only 53\% of the cloud pixels correct. This may not be surprising for several reasons. Firstly, there is class imbalance in the training set; of the labeled pixels, 60\% of them are ice, 40\% of them are cloud. With less cloud pixels to train on, it may be harder for the algorithms to predict cloud pixels accurately in the test set. Secondly, examining the density of the angular radiances in figure~\ref{fig:cloud/nocloud_rad_image1}, we remark that the icy pixels have much smaller variance in these features, potentially making them easier to identify. Conversely, the distribution of the features for cloudy pixels is much more spread out, even bimodal in the cases of BF, AF, and AN. Hence, there is more heterogeneity in the cloudy pixels, make them harder to classify. Finally, Tau et al.~\cite{shi2008daytime} remark that this heterogeneity can be attributed to the fact that clouds are non-stationary, and therefore measurements of clouds are noisier. Again, this may contribute the difficulty in predicting cloud cover. 


%From Figures \ref{fig:map_smo_features} and \ref{fig:map_orig_features}, we notice that in the upper left corner of image 3, there is a region corresponding to expert label +1. (The coordinate \texttt{x} is between 100 and 250, and \texttt{y} is between 200 and 350). All of our 3 algorithms didn't perform well in that region, where they misclassified most of the pixels into -1. 

All of our models were built purely on images 1 and 2, without opening image 3 for validation until we produce the report. Therefore, we expect that our predictive accuracies, obtained by testing on this held out third image, to be a fair representation of model performance. Assuming that image 3 is representative of future data and independent of image 1 and 2, it would be reasonable then to expect a similarly good performance on forthcoming data sets.

\printbibliography
\end{document}
